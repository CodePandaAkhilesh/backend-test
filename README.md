# ğŸ” LLM-Powered Intelligent Queryâ€“Retrieval System

This project is a backend API that uses **LLMs (Gemini)**, **LangChain**, and **Pinecone** to intelligently extract answers from large documents like PDFs, DOCX, and emails. It is designed to handle real-world queries in domains such as **insurance**, **legal**, **HR**, and **compliance**.

---

## ğŸš€ Features

- âœ… Accepts PDF 
- âœ… Extracts relevant sections based on semantic search  
- âœ… Parses and answers natural language questions  
- âœ… Provides structured and explainable JSON responses  
- âœ… Hosted on **Render**

---

## ğŸ§  Technologies Used

- **Google Gemini API** â€“ for natural language understanding and answer generation  
- **LangChain** â€“ for prompt orchestration and LLM integration  
- **Pinecone** â€“ for vector storage and semantic search  
- **Node.js + Express** â€“ for building the backend API  
- **pdf-parse** â€“ for extracting text from PDF files

---

## ğŸ“¦ API Endpoint

> **POST** `https://hackrx-backend-nv7c.onrender.com/hackrx/run`

### ğŸ”— Example Request Body

```json
{
  "documents": "https://example.com/your-document.pdf",
  "questions": [
    "Question 1....",
    "Question 2....",
    "Question 3....",
    "Question 4....",
    "Question 5....",
    "Question 6....",
    "Question 7....?",
    "Question 8....",
    "Question 9...",
    "Question 10...."
  ]
}
```

ğŸ“Œ Ensure your `documents` URL is **publicly accessible over HTTPS** (e.g., Dropbox, Google Drive with public link, or any static hosting service).

---

### ğŸ“¤ Sample Response

```json
{
  "answers": [
    "The waiting period for maternity expenses is 9 months.",
    "Yes, organ donor expenses are covered under the policy.",
    "The daily cash benefit is â‚¹1,000 per day for up to 30 days.",
    "..."
  ]
}
```

---

## ğŸ› ï¸ Setup (Local)

```bash
git clone https://github.com/yourusername/llm-query-retrieval.git
cd backend-test
npm install (add "type:module" in package.json file)
```

### ğŸ” Environment Variables

Create a `.env` file in the root directory with:

```env
GEMINI_API_KEY = Your Gemini Api Key
PINECONE_API_KEY = Your Pinecone Api Key
PINECONE_ENVIRONMENT = us-east-1 (default)
PINECONE_INDEX_NAME = Your Pinecone Index name
PORT=Port Number ( eg. 8080 )
API_TOKEN = Your secret token ( eg. testtoken123 )
```

### â–¶ï¸ Run Locally

```bash
npm run dev
```

---

## ğŸŒ Hosted Demo

Try it live:  
**POST** â†’ [`https://hackrx-backend-nv7c.onrender.com/hackrx/run`](https://hackrx-backend-nv7c.onrender.com/hackrx/run)

---

## ğŸ’¡ Problem Statement (HackRx 6.0)

> Design an LLM-Powered Intelligent Queryâ€“Retrieval System that can process large documents and make contextual decisions.  
> The system should handle real-world scenarios in **insurance**, **legal**, **HR**, and **compliance** domains.

---

---

## ğŸ§‘â€ğŸ’» Team & Contributions

This project was built as part of HackRx 6.0.

- ğŸ”§ **Akhilesh Verma** â€“ Developed and implemented the entire backend system.
- ğŸ“š **Other Team Members** â€“ Contributed by researching the problem statement, understanding domain use cases, and handling project submission.


## ğŸ“„ License

MIT License
